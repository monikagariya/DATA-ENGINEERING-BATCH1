{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecf6bd00-bc89-4d3e-88d5-70928b4201b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a281222-5f4f-43fb-b12e-526699de8614",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "collect_rdd = sc.parallelize([1,2,3,4,5])\n",
    "print(collect_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15b64299-8778-4f4c-9a30-265c203c8207",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "count_rdd = sc.parallelize([1,2,3,4,5,6,7,8,9])\n",
    "print(count_rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b03760b-0349-4a5a-99da-9ff9c9726de2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "first_rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "print(first_rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2d46755-b1f2-4773-ad8c-29ce3c75e988",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "take_rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "print(take_rdd.take(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "994f6a3c-e656-441d-b067-67a3f6ab7293",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "reduce_rdd = sc.parallelize([1,3,5,6,10])\n",
    "print(reduce_rdd.reduce(lambda x, y: x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "220f4855-a34e-439a-b76d-31f2d8de1790",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "save_rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "save_rdd.saveAsTextFile('file9.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0de3cbc-d58c-4389-bd9f-7a64e662c897",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 12, 13, 14]\n"
     ]
    }
   ],
   "source": [
    "#Transformations in PySpark RDDs  = The .map() Transformation\n",
    "\n",
    "my_rdd = sc.parallelize([1,2,3,4])\n",
    "print(my_rdd.map(lambda x: x+ 10).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b43c48b2-cd5f-4339-8abf-7dd2c21c8d74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "#The .filter() Transformation\n",
    "filter_rdd = sc.parallelize([2, 3, 4, 5, 6, 7])\n",
    "print(filter_rdd.filter(lambda x: x%2 == 0).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fea810c-aa83-433f-b223-7e4a2028822c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "#The .union() Transformation\n",
    "\n",
    "union_inp = sc.parallelize([2,4,5,6,7,8,9])\n",
    "union_rdd_1 = union_inp.filter(lambda x: x % 2 == 0)\n",
    "union_rdd_2 = union_inp.filter(lambda x: x % 3 == 0)\n",
    "print(union_rdd_1.union(union_rdd_2).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c78df9b2-74e7-4b26-94db-5438189d0c43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Hey', 'there', 'This', 'is', 'PySpark', 'RDD', 'Transformations']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The .flatMap() Transformation\n",
    "flatmap_rdd = sc.parallelize([\"Hey there\", \"This is PySpark RDD Transformations\"])\n",
    "(flatmap_rdd.flatMap(lambda x: x.split(\" \")).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1abb269a-9a8f-4bc6-823b-79cea3d41ea2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Rahul', 88), ('Swati', 92), ('Shreya', 83), ('Abhay', 93), ('Rohan', 78)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PySpark Pair RDD Operations\n",
    "\n",
    "marks = [('Rahul', 88), ('Swati', 92), ('Shreya', 83), ('Abhay', 93), ('Rohan', 78)]\n",
    "sc.parallelize(marks).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3579e249-d176-4a39-bb88-5ca5a050c9a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Shreya', 50), ('Swati', 45), ('Rahul', 48), ('Abhay', 55), ('Rohan', 44)]\n"
     ]
    }
   ],
   "source": [
    "#The .reduceByKey() Transformation\n",
    "marks_rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22), ('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
    "print(marks_rdd.reduceByKey(lambda x, y: x + y).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2faad15-8a92-4ec1-b06d-9f72c333de4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Abhay', 29), ('Abhay', 26), ('Rahul', 25), ('Rahul', 23), ('Rohan', 22), ('Rohan', 22), ('Shreya', 22), ('Shreya', 28), ('Swati', 26), ('Swati', 19)]\n"
     ]
    }
   ],
   "source": [
    "#The .sortByKey() Transformation\n",
    "marks_rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22), ('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
    "print(marks_rdd.sortByKey('ascending').collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f92aa98-3674-46df-8767-f06788278897",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shreya [22, 28]\nSwati [26, 19]\nRahul [25, 23]\nAbhay [29, 26]\nRohan [22, 22]\n"
     ]
    }
   ],
   "source": [
    "#The .groupByKey() Transformation\n",
    "\n",
    "marks_rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22), ('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
    "dict_rdd = marks_rdd.groupByKey().collect()\n",
    "for key, value in dict_rdd:\n",
    "    print(key, list(value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46411312-7a14-41e0-bc37-0934887fab3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rahul 2\nSwati 2\nRohan 2\nShreya 1\nAbhay 1\n"
     ]
    }
   ],
   "source": [
    "#Actions in Pair RDDs\n",
    "\n",
    "#The countByKey() Action\n",
    "marks_rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Rohan', 22), ('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
    "dict_rdd = marks_rdd.countByKey().items()\n",
    "for key, value in dict_rdd:\n",
    "    print(key, value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "842e99e2-b5bb-4240-866d-fa06a12a5744",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+------+\n|  Name|       DOB|Gender|salary|\n+------+----------+------+------+\n|   Ram|1991-04-01|     M|  3000|\n|  Mike|2000-05-19|     M|  4000|\n|Rohini|1978-09-05|     M|  4000|\n| Maria|1967-12-01|     F|  4000|\n| Jenis|1980-02-17|     F|  1200|\n+------+----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Create a spark session\n",
    "spark = SparkSession.builder.appName('pyspark - example join').getOrCreate()\n",
    "# Create data in dataframe\n",
    "data = [(('Ram'), '1991-04-01', 'M', 3000),\n",
    "       (('Mike'), '2000-05-19', 'M', 4000),\n",
    "       (('Rohini'), '1978-09-05', 'M', 4000),\n",
    "       (('Maria'), '1967-12-01', 'F', 4000),\n",
    "       (('Jenis'), '1980-02-17', 'F', 1200)] \n",
    "# Column names in dataframe\n",
    "columns = [\"Name\", \"DOB\", \"Gender\", \"salary\"]\n",
    " \n",
    "# Create the spark dataframe\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    " \n",
    "# Print the dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06aab89f-a72b-4b7e-a385-9a1e85d4228f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------+------+\n|  Name|DateOfBirth|Gender|salary|\n+------+-----------+------+------+\n|   Ram| 1991-04-01|     M|  3000|\n|  Mike| 2000-05-19|     M|  4000|\n|Rohini| 1978-09-05|     M|  4000|\n| Maria| 1967-12-01|     F|  4000|\n| Jenis| 1980-02-17|     F|  1200|\n+------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#Using withColumnRenamed()\n",
    "\n",
    "# Rename the column name from DOB to DateOfBirth\n",
    "# Print the dataframe\n",
    "df.withColumnRenamed(\"DOB\",\"DateOfBirth\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee0e558f-27c0-416e-9a4d-2ae6f5885b2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+------+\n|  Name|       DOB|Sex|Amount|\n+------+----------+---+------+\n|   Ram|1991-04-01|  M|  3000|\n|  Mike|2000-05-19|  M|  4000|\n|Rohini|1978-09-05|  M|  4000|\n| Maria|1967-12-01|  F|  4000|\n| Jenis|1980-02-17|  F|  1200|\n+------+----------+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "#Renaming multiple column names\n",
    "# Rename the column name 'Gender' to 'Sex'\n",
    "# Then for the returning dataframe \n",
    "# again rename the 'salary' to 'Amount'\n",
    "df.withColumnRenamed(\"Gender\",\"Sex\").withColumnRenamed(\"salary\",\"Amount\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "640dade1-2517-4b11-9667-d31464536793",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+------+\n|  name|       DOB|Gender|salary|\n+------+----------+------+------+\n|   Ram|1991-04-01|     M|  3000|\n|  Mike|2000-05-19|     M|  4000|\n|Rohini|1978-09-05|     M|  4000|\n| Maria|1967-12-01|     F|  4000|\n| Jenis|1980-02-17|     F|  1200|\n+------+----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#Using selectExpr()\n",
    "\n",
    "# Select the 'Name' as 'name'\n",
    "# Select remaining with their original name\n",
    "data = df.selectExpr(\"Name as name\",\"DOB\",\"Gender\",\"salary\")\n",
    " \n",
    "# Print the dataframe\n",
    "data.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8060b54-6f61-409d-9b5e-95907525afe2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+------+\n|  Name|       DOB|Gender|Amount|\n+------+----------+------+------+\n|   Ram|1991-04-01|     M|  3000|\n|  Mike|2000-05-19|     M|  4000|\n|Rohini|1978-09-05|     M|  4000|\n| Maria|1967-12-01|     F|  4000|\n| Jenis|1980-02-17|     F|  1200|\n+------+----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#Using select() method\n",
    "from pyspark.sql.functions import col\n",
    " \n",
    "# Select the 'salary' as 'Amount' using aliasing\n",
    "# Select remaining with their original name\n",
    "data = df.select(col(\"Name\"),col(\"DOB\"),\n",
    "                 col(\"Gender\"),\n",
    "                 col(\"salary\").alias('Amount'))\n",
    "\n",
    "data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6488527e-cf82-453a-b4ef-20906ddb5226",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------+-----------+\n|Emp Name|Date of Birth| Gender-m/f|Paid salary|\n+--------+-------------+-----------+-----------+\n|     Ram|   1991-04-01|          M|       3000|\n|    Mike|   2000-05-19|          M|       4000|\n|  Rohini|   1978-09-05|          M|       4000|\n|   Maria|   1967-12-01|          F|       4000|\n|   Jenis|   1980-02-17|          F|       1200|\n+--------+-------------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Using toDF()\n",
    "\n",
    "Data_list = [\"Emp Name\",\"Date of Birth\",\n",
    "             \" Gender-m/f\",\"Paid salary\"]\n",
    " \n",
    "new_df = df.toDF(*Data_list)\n",
    "new_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f624b85-3c63-4e1f-9f5c-b35731beda6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "6 Feb",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
